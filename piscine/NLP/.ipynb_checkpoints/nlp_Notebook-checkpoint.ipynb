{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43c5f87-8ca6-4420-a760-3b7c95d80585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import jupyter\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06944dde-4cc9-418c-ab38-555484c5a818",
   "metadata": {},
   "source": [
    "## Exercise 1: Lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9917e933-5b88-44ae-9997-74f2fcc4392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    this is my first nlp exercise\n",
       "1                         wtf!!!!!\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ = [\"This is my first NLP exercise\", \"wtf!!!!!\"]\n",
    "series_data = pd.Series(list_, name='text')\n",
    "\n",
    "# Convert the series data to lowercase and uppercase\n",
    "lowercase_texts = series_data.str.lower()\n",
    "\n",
    "lowercase_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d7370d-2f41-4a60-a08d-325436dc840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    THIS IS MY FIRST NLP EXERCISE\n",
       "1                         WTF!!!!!\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercase_texts = series_data.str.upper()\n",
    "\n",
    "uppercase_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3fd6b-da66-4441-84a8-8c35d23d521d",
   "metadata": {},
   "source": [
    "## Exercise 2: Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5c6f9a-058e-496b-9596-3e8f2f80236b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remove this from  the sentence  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given sentence\n",
    "sentence = \"Remove, this from .? the sentence !!!! !\\\"#&'()*+,-./:;<=>_\"\n",
    "\n",
    "# Remove punctuation\n",
    "sentence_no_punctuation = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "sentence_no_punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820696d-285b-44f5-b9ff-33a4284e36e4",
   "metadata": {},
   "source": [
    "## Exercise 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df4111ad-d7e3-498a-bce1-1008e9a5cb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto.',\n",
       " 'The currency began use in 2009 when its implementation was released as open-source software.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Given text\n",
    "text = \"\"\"Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\"\"\"\n",
    "\n",
    "# Tokenize the text by sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize the text by words\n",
    "\n",
    "\n",
    "sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63de3dbb-382f-4f6a-87dc-73bd1f6e6aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bitcoin',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cryptocurrency',\n",
       " 'invented',\n",
       " 'in',\n",
       " '2008',\n",
       " 'by',\n",
       " 'an',\n",
       " 'unknown',\n",
       " 'person',\n",
       " 'or',\n",
       " 'group',\n",
       " 'of',\n",
       " 'people',\n",
       " 'using',\n",
       " 'the',\n",
       " 'name',\n",
       " 'Satoshi',\n",
       " 'Nakamoto',\n",
       " '.',\n",
       " 'The',\n",
       " 'currency',\n",
       " 'began',\n",
       " 'use',\n",
       " 'in',\n",
       " '2009',\n",
       " 'when',\n",
       " 'its',\n",
       " 'implementation',\n",
       " 'was',\n",
       " 'released',\n",
       " 'as',\n",
       " 'open-source',\n",
       " 'software',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "\n",
    "words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72d472-4749-4e39-bdcc-486fbb8646b0",
   "metadata": {},
   "source": [
    "## Exercise 4: Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6a8af3-f1f9-479f-88cf-43e5e35dcee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['goal', 'exercise', 'learn', 'remove', 'stop', 'words', 'NLTK', '.', 'Stop', 'words', 'usually', 'refers', 'common', 'words', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Given text\n",
    "text = \"\"\"\n",
    "The goal of this exercise is to learn to remove stop words with NLTK. Stop words usually refers to the most common words in a language.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from the tokenized words\n",
    "filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d325d045-efbe-4319-adbc-25d09e71bd2f",
   "metadata": {},
   "source": [
    "## Exercise 5: Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "745db834-4d68-4627-bb5b-1de67a53249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'interview', 'interview', 'the', 'presid', 'in', 'an', 'interview']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The interviewer interviews the president in an interview\"\n",
    "\n",
    "# Initialize the Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stemmed_words = [ps.stem(word) for word in words]\n",
    "\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4218ef-f60a-4ef0-bf06-1c4f127e9a11",
   "metadata": {},
   "source": [
    "## Exercise 6: Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d899129-28c8-4dec-b526-203a92ede2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01',\n",
       " 'edu',\n",
       " 'system',\n",
       " 'present',\n",
       " 'innov',\n",
       " 'curriculum',\n",
       " 'softwar',\n",
       " 'engin',\n",
       " 'program',\n",
       " 'renown',\n",
       " 'industrylead',\n",
       " 'reput',\n",
       " 'curriculum',\n",
       " 'rigor',\n",
       " 'design',\n",
       " 'learn',\n",
       " 'skill',\n",
       " 'digit',\n",
       " 'world',\n",
       " 'technolog',\n",
       " 'industri',\n",
       " 'take',\n",
       " 'differ',\n",
       " 'approach',\n",
       " 'classic',\n",
       " 'teach',\n",
       " 'method',\n",
       " 'today',\n",
       " 'learn',\n",
       " 'facilit',\n",
       " 'collect',\n",
       " 'cocr',\n",
       " 'process',\n",
       " 'profession',\n",
       " 'environ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filter out stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "text = \"01 Edu System presents an innovative curriculum in software engineering and programming. With a renowned industry-leading reputation, the curriculum has been rigorously designed for learning skills of the digital world and technology industry. Taking a different approach than the classic teaching methods today, learning is facilitated through a collective and co-creative process in a professional environment.\"\n",
    "proccessed_text = preprocess_text(text)\n",
    "proccessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc090e-4bdc-42f4-811c-ffbd2b103a1d",
   "metadata": {},
   "source": [
    "## Exercise 7: Bag of Word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84bbd62-7efb-47ed-821f-8ffe2dd9638f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6588x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 37334 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = []\n",
    "with open(\"tweets_train.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Each line is a tweet, so we'll preprocess each one and append it to our tweets list.\n",
    "        # The file format includes the sentiment at the beginning, so we'll split on the first comma to separate it.\n",
    "        sentiment, tweet_text = line.split(',', 1)\n",
    "        tweets.append((sentiment.strip(), preprocess_text(tweet_text.strip())))\n",
    "\n",
    "tweets_text = [\" \".join(tweet[1]) for tweet in tweets]\n",
    "sentiments = [tweet[0] for tweet in tweets]\n",
    "\n",
    "# Initialize CountVectorizer with max_features=500\n",
    "vectorizer = CountVectorizer(max_features=500)\n",
    "\n",
    "# Fit the vectorizer to the tweets and transform the data\n",
    "transformed_tweets = vectorizer.fit_transform(tweets_text)\n",
    "\n",
    "# Check the shape of the word count matrix\n",
    "transformed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c38187-bbb1-4af4-8ee5-6d7b40d3153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    |   someth |   son |   song |\n",
      "|---:|---------:|------:|-------:|\n",
      "|  0 |        0 |     0 |      0 |\n",
      "|  1 |        0 |     0 |      0 |\n",
      "|  2 |        0 |     0 |      0 |\n"
     ]
    }
   ],
   "source": [
    "count_vectorized_df = pd.DataFrame.sparse.from_spmatrix(transformed_tweets)\n",
    "\n",
    "# Retrieve feature names from the vectorizer and set them as column names\n",
    "count_vectorized_df.columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Optionally, if you have the sentiments and wish to include them as a column in the DataFrame\n",
    "count_vectorized_df['sentiment'] = sentiments\n",
    "\n",
    "print(count_vectorized_df.iloc[:3,400:403].to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e807b194-3234-48eb-802d-8255460b09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant    1\n",
      "deal    1\n",
      "end     1\n",
      "find    1\n",
      "keep    1\n",
      "like    1\n",
      "may     1\n",
      "say     1\n",
      "talk    1\n",
      "Name: 3, dtype: Sparse[object, 0]\n"
     ]
    }
   ],
   "source": [
    "fourth_tweet_token_counts = count_vectorized_df.iloc[3]\n",
    "\n",
    "\n",
    "fourth_tweet_token_counts_1 = fourth_tweet_token_counts[fourth_tweet_token_counts == 1]\n",
    "\n",
    "# Display the filtered token counts\n",
    "print(fourth_tweet_token_counts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f81c452-8ded-4836-a9f4-0eae3445f3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tomorrow    1126\n",
       "go           733\n",
       "day          667\n",
       "night        641\n",
       "may          533\n",
       "tonight      501\n",
       "see          439\n",
       "time         429\n",
       "im           422\n",
       "get          398\n",
       "today        389\n",
       "game         382\n",
       "saturday     379\n",
       "friday       375\n",
       "sunday       368\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the sums in descending order to get the most frequent tokens\n",
    "\n",
    "\n",
    "token_frequencies = count_vectorized_df.drop(columns=['sentiment']).sum().sort_values(ascending=False)\n",
    "\n",
    "# Select the 15 most used tokens\n",
    "top_15_tokens = token_frequencies.head(15)\n",
    "\n",
    "top_15_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf0d611-f2b5-46df-b84d-41d2ad259efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     |   your | sentiment   |   label |\n",
      "|----:|-------:|:------------|--------:|\n",
      "| 350 |      0 | positive    |       1 |\n",
      "| 351 |      1 | negative    |      -1 |\n",
      "| 352 |      0 | positive    |       1 |\n",
      "| 353 |      0 | neutral     |       0 |\n"
     ]
    }
   ],
   "source": [
    "count_vectorized_df['label'] = sentiments\n",
    "sentiment_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "count_vectorized_df['label'] = count_vectorized_df['label'].map(sentiment_mapping)\n",
    "\n",
    "count_vectorized_df\n",
    "print(count_vectorized_df.iloc[350:354,499:502].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c44b-b67a-4342-af3f-22ca46916ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
